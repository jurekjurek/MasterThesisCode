{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We explore the Pennylane model that lambeq provides. \n",
    "\n",
    "'''\n",
    "\n",
    "# import things \n",
    "import torch \n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 12\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "It seems that the sentence similarity task has already been done before with the datasets of interest. \n",
    "\n",
    "'''\n",
    "\n",
    "# load data \n",
    "\n",
    "def read_data(filename):\n",
    "    labels, sentences = [], []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            line = line.split(',')\n",
    "            labels.append(int(line[2]))\n",
    "            sentences.append((line[0], line[1]))\n",
    "    return labels, sentences\n",
    "\n",
    "train_labels, train_data = read_data('../datasets/mc_pair_train_data.csv')\n",
    "dev_labels, dev_data = read_data('../datasets/mc_pair_dev_data.csv')\n",
    "test_labels, test_data = read_data('../datasets/mc_pair_test_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tagging sentences.\n",
      "Parsing tagged sentences.\n",
      "Turning parse trees to diagrams.\n",
      "Tagging sentences.\n",
      "Parsing tagged sentences.\n",
      "Turning parse trees to diagrams.\n",
      "Tagging sentences.\n",
      "Parsing tagged sentences.\n",
      "Turning parse trees to diagrams.\n"
     ]
    }
   ],
   "source": [
    "# create diagrams \n",
    "\n",
    "# take pairs apart for now, repair them later! \n",
    "\n",
    "train_data_l, train_data_r = zip(*train_data)\n",
    "train_data_unpaired = list(train_data_l) + list(train_data_r)\n",
    "dev_data_l, dev_data_r = zip(*dev_data)\n",
    "dev_data_unpaired = list(dev_data_l) + list(dev_data_r)\n",
    "test_data_l, test_data_r = zip(*test_data)\n",
    "test_data_unpaired = list(test_data_l) + list(test_data_r)\n",
    "from lambeq import BobcatParser\n",
    "\n",
    "reader = BobcatParser(verbose='text')\n",
    "\n",
    "raw_train_diagrams = reader.sentences2diagrams(train_data_unpaired)\n",
    "raw_dev_diagrams = reader.sentences2diagrams(dev_data_unpaired)\n",
    "raw_test_diagrams = reader.sentences2diagrams(test_data_unpaired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplify diagrams by removing cups \n",
    "\n",
    "from lambeq import RemoveCupsRewriter\n",
    "\n",
    "remove_cups = RemoveCupsRewriter()\n",
    "\n",
    "train_diagrams = [remove_cups(diagram) for diagram in raw_train_diagrams]\n",
    "dev_diagrams = [remove_cups(diagram) for diagram in raw_dev_diagrams]\n",
    "test_diagrams = [remove_cups(diagram) for diagram in raw_test_diagrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create circuits \n",
    "'''\n",
    "\n",
    "from lambeq import AtomicType, IQPAnsatz\n",
    "\n",
    "ansatz = IQPAnsatz({AtomicType.NOUN: 1, AtomicType.SENTENCE: 1},\n",
    "                   n_layers=1, n_single_qubit_params=3)\n",
    "\n",
    "train_circuits = [ansatz(diagram) for diagram in train_diagrams]\n",
    "dev_circuits =  [ansatz(diagram) for diagram in dev_diagrams]\n",
    "test_circuits = [ansatz(diagram) for diagram in test_diagrams]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "EPOCHS = 100\n",
    "SEED = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from lambeq import PennyLaneModel\n",
    "\n",
    "# inherit from PennyLaneModel to use the PennyLane circuit evaluation\n",
    "class XORSentenceModel(PennyLaneModel):\n",
    "    def __init__(self, **kwargs):\n",
    "        PennyLaneModel.__init__(self, **kwargs)\n",
    "\n",
    "        self.xor_net = nn.Sequential(nn.Linear(4, 10),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(10, 1),\n",
    "                                     nn.Sigmoid())\n",
    "\n",
    "    def forward(self, diagram_pairs):\n",
    "        first_d, second_d = zip(*diagram_pairs)\n",
    "        # evaluate each circuit and concatenate the results\n",
    "        evaluated_pairs = torch.cat((self.get_diagram_output(first_d),\n",
    "                                     self.get_diagram_output(second_d)),\n",
    "                                    dim=1)\n",
    "        evaluated_pairs = 2 * (evaluated_pairs - 0.5)\n",
    "        # pass the concatenated results through a simple neural network\n",
    "        return self.xor_net(evaluated_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create paired data\n",
    "'''\n",
    "\n",
    "def make_pair_data(diagrams):\n",
    "    pair_diags = list(zip(diagrams[:len(diagrams)//2], diagrams[len(diagrams)//2:]))\n",
    "    return pair_diags\n",
    "\n",
    "train_pair_circuits = make_pair_data(train_circuits)\n",
    "dev_pair_circuits = make_pair_data(dev_circuits)\n",
    "test_pair_circuits = make_pair_data(test_circuits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq import Dataset\n",
    "# from lambeq import PennyLaneModel\n",
    "\n",
    "\n",
    "\n",
    "all_pair_circuits = (train_pair_circuits +\n",
    "                     dev_pair_circuits +\n",
    "                     test_pair_circuits)\n",
    "a, b = zip(*all_pair_circuits)\n",
    "\n",
    "# initialise our model by passing in the diagrams, so that we have trainable parameters for each token\n",
    "model = XORSentenceModel.from_diagrams(a + b, probabilities=True, normalize=True)\n",
    "model.initialise_weights()\n",
    "model = model.double()\n",
    "\n",
    "# initialise datasets and optimizers as in PyTorch\n",
    "train_pair_dataset = Dataset(train_pair_circuits,\n",
    "                             train_labels,\n",
    "                             batch_size=BATCH_SIZE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(circs, labels):\n",
    "    predicted = model(circs)\n",
    "    return (torch.round(torch.flatten(predicted)) ==\n",
    "            torch.DoubleTensor(labels)).sum().item()/len(circs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train loss: 4.16150390641149\n",
      "Dev acc: 0.515\n",
      "Epoch: 5\n",
      "Train loss: 4.129963023137189\n",
      "Dev acc: 0.495\n",
      "Epoch: 10\n",
      "Train loss: 4.162349233298943\n",
      "Dev acc: 0.535\n",
      "Epoch: 15\n",
      "Train loss: 4.084082146218643\n",
      "Dev acc: 0.52\n",
      "Epoch: 20\n",
      "Train loss: 4.059490078564071\n",
      "Dev acc: 0.485\n",
      "Epoch: 25\n",
      "Train loss: 4.132734674241028\n",
      "Dev acc: 0.5\n",
      "Epoch: 30\n",
      "Train loss: 3.9851123160285646\n",
      "Dev acc: 0.53\n",
      "Epoch: 35\n",
      "Train loss: 3.972572400297392\n",
      "Dev acc: 0.495\n",
      "Epoch: 40\n",
      "Train loss: 4.042745903227465\n",
      "Dev acc: 0.505\n",
      "Epoch: 45\n",
      "Train loss: 3.9525597265521024\n",
      "Dev acc: 0.505\n",
      "Epoch: 50\n",
      "Train loss: 4.0093337579081965\n",
      "Dev acc: 0.53\n",
      "Epoch: 55\n",
      "Train loss: 4.110671271610174\n",
      "Dev acc: 0.525\n",
      "Epoch: 60\n",
      "Train loss: 4.010737598836101\n",
      "Dev acc: 0.5\n",
      "Epoch: 65\n",
      "Train loss: 4.012300672298847\n",
      "Dev acc: 0.535\n",
      "Epoch: 70\n",
      "Train loss: 3.9552167669384435\n",
      "Dev acc: 0.485\n",
      "Epoch: 75\n",
      "Train loss: 3.967403234137479\n",
      "Dev acc: 0.5\n",
      "Epoch: 80\n",
      "Train loss: 3.9566375532049483\n",
      "Dev acc: 0.525\n",
      "Epoch: 85\n",
      "Train loss: 3.962031198242382\n",
      "Dev acc: 0.49\n",
      "Epoch: 90\n",
      "Train loss: 3.9979754817001636\n",
      "Dev acc: 0.52\n",
      "Epoch: 95\n",
      "Train loss: 3.9267022951749064\n",
      "Dev acc: 0.505\n"
     ]
    }
   ],
   "source": [
    "best = {'acc': 0, 'epoch': 0}\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for circuits, labels in train_pair_dataset:\n",
    "        optimizer.zero_grad()\n",
    "        predicted = model(circuits)\n",
    "        # use BCELoss as our outputs are probabilities, and labels are binary\n",
    "        loss = torch.nn.functional.binary_cross_entropy(\n",
    "            torch.flatten(predicted), torch.DoubleTensor(labels))\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # evaluate on dev set every 5 epochs\n",
    "    # save the model if it's the best so far\n",
    "    # stop training if the model hasn't improved for 10 epochs\n",
    "    if i % 5 == 0:\n",
    "        dev_acc = accuracy(dev_pair_circuits, dev_labels)\n",
    "\n",
    "        print('Epoch: {}'.format(i))\n",
    "        print('Train loss: {}'.format(epoch_loss))\n",
    "        print('Dev acc: {}'.format(dev_acc))\n",
    "\n",
    "        if dev_acc > best['acc']:\n",
    "            best['acc'] = dev_acc\n",
    "            best['epoch'] = i\n",
    "            model.save('xor_model.lt')\n",
    "        # elif i - best['epoch'] >= 10:\n",
    "        #     print('Early stopping')\n",
    "        #     break\n",
    "\n",
    "# load the best performing iteration of the model on the dev set\n",
    "if best['acc'] > accuracy(dev_pair_circuits, dev_labels):\n",
    "    model.load('xor_model.lt')\n",
    "    model = model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy: 0.545\n"
     ]
    }
   ],
   "source": [
    "print('Final test accuracy: {}'.format(accuracy(test_pair_circuits,\n",
    "                                                test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46725587],\n",
       "       [0.4959365 ],\n",
       "       [0.46433353],\n",
       "       [0.50692673]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xor_labels = [[1, 0, 1, 0], [0, 1, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]]\n",
    "# the first two entries correspond to the same label for both sentences,\n",
    "# the last two to different labels\n",
    "xor_tensors = torch.tensor(xor_labels).double()\n",
    "\n",
    "model.xor_net(xor_tensors).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skillful person prepares software .\n",
      "[0.30974691 0.69025309]\n"
     ]
    }
   ],
   "source": [
    "# cooking sentence\n",
    "print(test_data[1][0])\n",
    "\n",
    "p_circ = test_pair_circuits[0][0].to_pennylane(probabilities=True)\n",
    "symbol_weight_map = dict(zip(model.symbols, model.weights))\n",
    "p_circ.initialise_concrete_params(symbol_weight_map)\n",
    "unnorm = p_circ.eval().detach().numpy()\n",
    "\n",
    "print(unnorm / np.sum(unnorm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
